## 计算bert的参数量
- 参考同学的总结图示
- 参考老师diy的bert代码

### Embedding
#### 步骤
- 根据tocken获取三个Embedding权重
- 三个Embedding权重相加
- 进行LayerNorm归一化

#### 三个Embedding权重的总量
- 词表权重总量为[21128 * 768]
- 位置权重总量为[512 * 768]
- 类别权重总量为[2 * 768]

#### 每个Token的权重量
- Token只是根据index从总量中选取相应的权重
- Token的权重量为[768] + [768] + [768]
- 从位置权重总量看最多有512个Token进行注意力计算
- 从类别权重总量看最多有2个类别来确定边界

#### 归一化计算权重
- 归一化输入是Token权重相加
- 归一化权重为weight[768] + bias[768]

### Transformer
- 包括自注意力层和前馈神经网络层
- Transformer会被叠加多次使用,Bert是12次

#### 自注意力层
- 自注意力层的输入一个Token[768]
- 每个Token经过[Q,K,V]三个线性层，其中[Q,K,V]分别是[768*768, 768*768, 768*768]
- 输出为[q,k,v],其中[q,k,v]是[768, 768, 768]
- q*k后再softmax后与v相乘,得到[768]
- qkv再经过一个线性层得到[768*768] + [768]
- 其中12多头是指将[Q,K,V]拆分成12个[768*64]
- 输出[768]再经过残差机制，与输入相加后再经过一个归一化层到[768] + [768]

#### 前馈神经网络层
- 输入[768]通过线性层到[768*3072] + [3072]扩大参数量到[3072]
- 再经过一个激活函数GELU
- 再经过一个线性层到[3072*768]恢复参数量[768]
- 输出[768]再经过残差机制，与输入相加后再经过一个归一化层到[768] + [768]

### Pooler
- 经过12层Transformer后得到[768]
- 再经过一个线性层到[768*768] + [768]
- 得到[768]后进行tanh计算
